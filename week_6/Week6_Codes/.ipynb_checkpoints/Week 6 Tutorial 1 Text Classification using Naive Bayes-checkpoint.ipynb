{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "Text Classification is one of the widely use from a task of natural language processing (NLP). It is also one of the important and typical task in pattern recognition. Text classification aims to automatically classify text documents into one or more defined categories which  can be a web page, library book, media articles, gallery etc. Some examples of text classification can be:\n",
    "\n",
    "- Understanding audience sentiment from social media,\n",
    "- Detection of spam and non-spam emails,\n",
    "- Auto tagging of customer queries, and\n",
    "- Categorization of news articles into defined topics.\n",
    "\n",
    "In this tutorial, we would like to demonstrate how we can categorize different new articles into defined topics using sklearn library. \n",
    "\n",
    "# Dataset\n",
    "\n",
    "The dataset used is the 20 Newsgroup dataset (http://qwone.com/~jason/20Newsgroups/). The 20 Newsgroups data set is a collection of about 20000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. The dataset was originally collected by Ken Lang. The 20 newsgroups dataset has become a popular data for experiments in text applications of machine learning techniques including text classification and text clustering. \n",
    "\n",
    "The dataset used in this tutorial is extracted from the 20 Newsgroups dataset sorted by date in which duplicates and some headers removed. The dataset contains 18846 documents. The data is categorized into 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware / comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale / soc.religion.christian). Here is a list of the 20 newsgroups, partitioned (more or less) according to subject matter:\n",
    "\n",
    "- comp.graphics\n",
    "- comp.os.ms-windows.misc\n",
    "- comp.sys.ibm.pc.hardware\n",
    "- comp.sys.mac.hardware\n",
    "- comp.windows.x\t\n",
    "- rec.autos\n",
    "- rec.motorcycles\n",
    "- rec.sport.baseball\n",
    "- rec.sport.hockey\t\n",
    "- sci.crypt\n",
    "- sci.electronics\n",
    "- sci.med\n",
    "- sci.space\n",
    "- misc.forsale\t\n",
    "- talk.politics.misc\n",
    "- talk.politics.guns\n",
    "- talk.politics.mideast\t\n",
    "- talk.religion.misc\n",
    "- alt.atheism\n",
    "- soc.religion.christian\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of news articles into different topics using Naive Bayes models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "#categories = ['alt.atheism', 'talk.religion.misc','comp.graphics', 'sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
    "newsgroups = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)\\nSubject: Which high-performance VLB video card?\\nSummary: Seek recommendations for VLB video card\\nNntp-Posting-Host: midway.ecn.uoknor.edu\\nOrganization: Engineering Computer Network, University of Oklahoma, Norman, OK, USA\\nKeywords: orchid, stealth, vlb\\nLines: 21\\n\\n  My brother is in the market for a high-performance video card that supports\\nVESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\\n\\n  - Diamond Stealth Pro Local Bus\\n\\n  - Orchid Farenheit 1280\\n\\n  - ATI Graphics Ultra Pro\\n\\n  - Any other high-performance VLB card\\n\\n\\nPlease post or email.  Thank you!\\n\\n  - Matt\\n\\n-- \\n    |  Matthew B. Lawson <------------> (mblawson@essex.ecn.uoknor.edu)  |   \\n  --+-- \"Now I, Nebuchadnezzar, praise and exalt and glorify the King  --+-- \\n    |   of heaven, because everything he does is right and all his ways  |   \\n    |   are just.\" - Nebuchadnezzar, king of Babylon, 562 B.C.           |   \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(newsgroups.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.filenames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "This step will calculate a bag of words and their frequencies. We use the CountVectorizer from sklearn to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the CountVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see an example first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'and': 0,\n",
       " 'third': 7,\n",
       " 'one': 4}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit_transform(document)\n",
    "\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(document).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output a bag of words and their frequencey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'document': 4, 'first': 2, 'is': 4, 'one': 1, 'second': 1, 'the': 4, 'third': 1, 'this': 4}\n"
     ]
    }
   ],
   "source": [
    "word_list = cv.get_feature_names()\n",
    "count_list = cv.fit_transform(document).toarray().sum(axis=0)\n",
    "\n",
    "print(dict(zip(word_list,count_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction for the 20NewsGroup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cv.fit_transform(newsgroups.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 173762)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split these feature vectors into a trainning and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, \n",
    "                                                    newsgroups.target, \n",
    "                                                    train_size = 0.8,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15076, 173762)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3770, 173762)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15076,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3770,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Multinomial Naive Bayes model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB(alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model and make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  6 10 ... 13 12 13]\n",
      "[12 13 10 ... 13  3 13]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8557029177718833\n",
      "Precision: 0.8727827934235892\n",
      "Recall: 0.8557029177718833\n",
      "F1-score: 0.8444185008277632\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"F1-score:\",metrics.f1_score(y_test, y_pred, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred, target_names = newsgroups.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.86      0.94      0.90       163\n",
      "           comp.graphics       0.67      0.92      0.77       190\n",
      " comp.os.ms-windows.misc       0.98      0.21      0.35       200\n",
      "comp.sys.ibm.pc.hardware       0.64      0.83      0.72       196\n",
      "   comp.sys.mac.hardware       0.88      0.86      0.87       201\n",
      "          comp.windows.x       0.74      0.90      0.81       198\n",
      "            misc.forsale       0.94      0.71      0.81       206\n",
      "               rec.autos       0.90      0.94      0.92       177\n",
      "         rec.motorcycles       0.99      0.90      0.94       189\n",
      "      rec.sport.baseball       0.94      0.98      0.96       171\n",
      "        rec.sport.hockey       0.97      0.96      0.97       233\n",
      "               sci.crypt       0.83      0.98      0.90       190\n",
      "         sci.electronics       0.88      0.80      0.84       207\n",
      "                 sci.med       0.95      0.91      0.93       203\n",
      "               sci.space       0.89      0.95      0.92       191\n",
      "  soc.religion.christian       0.80      0.95      0.87       198\n",
      "      talk.politics.guns       0.86      0.96      0.91       155\n",
      "   talk.politics.mideast       0.92      0.99      0.95       196\n",
      "      talk.politics.misc       0.89      0.90      0.89       170\n",
      "      talk.religion.misc       0.94      0.48      0.63       136\n",
      "\n",
      "                accuracy                           0.86      3770\n",
      "               macro avg       0.87      0.85      0.84      3770\n",
      "            weighted avg       0.87      0.86      0.84      3770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[153,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,   0,\n",
       "          0,   1,   4,   0,   0,   0,   4],\n",
       "       [  0, 175,   0,   6,   1,   1,   1,   0,   0,   0,   0,   3,   0,\n",
       "          0,   1,   0,   0,   1,   1,   0],\n",
       "       [  0,  32,  43,  50,   7,  47,   0,   1,   0,   1,   0,   6,   5,\n",
       "          1,   3,   1,   1,   0,   2,   0],\n",
       "       [  0,  10,   0, 163,   9,   4,   3,   0,   0,   0,   1,   4,   1,\n",
       "          0,   1,   0,   0,   0,   0,   0],\n",
       "       [  1,   4,   0,   9, 172,   4,   1,   0,   0,   0,   0,   2,   6,\n",
       "          0,   1,   0,   0,   1,   0,   0],\n",
       "       [  0,  12,   1,   4,   0, 178,   0,   0,   0,   0,   0,   2,   0,\n",
       "          0,   1,   0,   0,   0,   0,   0],\n",
       "       [  0,   8,   0,  11,   3,   3, 147,   8,   1,   2,   0,   6,   7,\n",
       "          2,   3,   1,   1,   2,   1,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   2, 166,   1,   2,   0,   0,   2,\n",
       "          0,   0,   0,   2,   0,   2,   0],\n",
       "       [  0,   0,   0,   1,   0,   1,   1,   7, 170,   1,   0,   0,   0,\n",
       "          0,   2,   1,   1,   1,   3,   0],\n",
       "       [  0,   1,   0,   0,   0,   0,   0,   0,   0, 167,   2,   0,   0,\n",
       "          0,   0,   1,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4, 224,   0,   0,\n",
       "          0,   0,   2,   1,   1,   1,   0],\n",
       "       [  0,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0, 186,   0,\n",
       "          0,   0,   0,   0,   0,   2,   0],\n",
       "       [  0,  13,   0,   8,   4,   0,   1,   1,   0,   0,   1,  10, 165,\n",
       "          0,   4,   0,   0,   0,   0,   0],\n",
       "       [  1,   4,   0,   2,   0,   0,   0,   1,   0,   0,   0,   0,   2,\n",
       "        185,   4,   2,   2,   0,   0,   0],\n",
       "       [  0,   1,   0,   0,   0,   1,   1,   0,   0,   0,   0,   0,   0,\n",
       "          1, 182,   2,   0,   2,   1,   0],\n",
       "       [  2,   0,   0,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          4,   0, 189,   0,   1,   1,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
       "          0,   0,   1, 149,   2,   2,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,   0,\n",
       "          0,   0,   1,   0, 194,   0,   0],\n",
       "       [  0,   1,   0,   0,   0,   0,   0,   0,   0,   1,   0,   3,   0,\n",
       "          0,   1,   0,   7,   4, 153,   0],\n",
       "       [ 21,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
       "          2,   1,  32,   9,   2,   3,  65]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best parameter alpha for the model using GridSearchCV \n",
    "\n",
    "GridSearchCV is an exhaustive search with cross validation over specified parameter values to determine the optimal value for a given model. The parameters of the model used to apply these methods are optimized by cross-validated grid-search over a parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Grid Search\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters setting for alpha = 10, 1, 0.1, 0.001, 0.0001**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'alpha': [10, 1, 1e-1, 1e-2, 1e-3]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the Grid Search on parameters and fit the trainning data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold\n",
    "#grs = GridSearchCV(model, param_grid=params, cv = 10)\n",
    "\n",
    "# 5-fold default\n",
    "grs = GridSearchCV(model, param_grid=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=MultinomialNB(),\n",
       "             param_grid={'alpha': [10, 1, 0.1, 0.01, 0.001]})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output the optimal values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters: {'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyper Parameters:\",grs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make a prediction and calculate metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=grs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8949602122015915\n",
      "Precision: 0.9011003567458483\n",
      "Recall: 0.8949602122015915\n",
      "F1-score: 0.893045682368305\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred, average = 'weighted'))\n",
    "print(\"F1-score:\",metrics.f1_score(y_test, y_pred, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
